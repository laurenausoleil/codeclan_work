---
title: "R Notebook"
output: html_notebook
---

# Categories of variable reduction methods

1. Filter methods
Ranking variables through descriptive measures, such as correlation with target variable.

2. Wrapper methods
building models with different subsets of variables and computing usefulness from the models performance.
Forward selection - start with no variables and add them until there is no significant improvement.
Backward selection - start with all variabls and remove them one by one if the performance of the model does not decrease significantly.

3. Embedded Methods
e.g. lasso regression and ridge regression
the method used to reduce the number of features is built into the modelling algorithm. 

# Dimensionality Reduction

Dimensionality reduction involves calculating a basis or mathematical representation within which you can describe most but not all of the variance within your data, thereby retaining the relevant information while reducing the number of dimensions necessary to represent it. 
There are a variety of techniques for doing this including: PCA (principle components analysis), ICA (independent component analysis), and LDA (linear discriminant analysis).

# Principal Component Analysis
Summarise multiple variables into fewer variables called components.
Components are generated by looking to the variance between variables in order to retain useful information and drop less valuable information.

PCA is not neccessary for modelling

Use Case:
* you need less variables, but can't spot the right ones manually
* you want to ensure variables are independent
* you are happy to work with variables that are less easily interpreted and less communicable
* you are working with numerical data
* your data is standardised/normalised and scaled appropriately.

# e.g. PCA on Mtcars
```{r}
library(tidyverse)
```

Explore the data
```{r}
names(mtcars)
```
```{r}
head(mtcars, 1)
```

Remove non-numeric variables including categorical variables
```{r}
cars_numeric <- mtcars %>% 
  select(-c(vs,am))
```

Perform PCA with prcomp()
prcomp() performs singular value decomposition to find eigenvectors and eigenvariables, then defines the directions of variance in the data and how much of the variance that direction accounts for.
```{r}
cars_pca <- prcomp(cars_numeric, center = TRUE, scale. = TRUE)
summary(cars_pca)
```

We now have 9 dimensions (our dimensions have not been reduced).
The proportion of variance row tells us the r squared for each dimension.
Check that cumulative proportion adds up to 100. If it doesn't you have likely forgotten to scale.

We can look at our pca in more detail
```{r}
str(cars_pca)
```

Rotation shows us the make-up of our new principal components
```{r}
cars_pca$rotation
```
PC1 is made up of positive contributions from cyl, disp, hp, wt and carb and negative contributions for mpg, drat, gear and qsec.

# Visualise PCA with biplot- not currently working.

Install library from github
```{r}
library(devtools)
install_github("vqv/ggbiplot", ref = "experimental", force = TRUE)
library(ggbiplot)
```

# Manual variable reduction can be done with common sense or alias.
