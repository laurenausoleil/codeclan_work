
```{r}
library(tidyverse)
library(tidytext)
library(janeaustenr)
```

```{r}
sentences <- c(
  "This is a sentence about cats.",
  "This is a sentence about dogs.",
  "This is a sentence about alligators."
)
```

```{r}
sentences_df <- tibble(
    sentence = sentences,
    id = 1:length(sentences)
  ) %>% 
  unnest_tokens(word, sentence)

sentences_df
```

```{r}
sentences_df %>% 
  count(word, id)
```

# TF-IDF
Term Frequency - Inverse Document Frequency
Measures how important a word is to a document in a collection of documents.
* identify what different documents are about
* identify distinctions between documents

## Methodology
A method for weighting words. Words which are common in many places have a lower weight. Words which are uncommon have higher weights.
Term Frequency = how often term appears in document / number of terms in document
e.g. Cats = 1/6
Document Frquency = number of documents term appears in / number of documents
e.g. Cats = 1/3
TF-IDF = TF * log(1/DF)

## Practice

```{r}
sentences_df %>% 
  count(word, id) %>% 
  bind_tf_idf(
    term = word,
    document = id,
    n = n
  ) %>% 
  arrange(desc(tf_idf))
```
The tf-idfs of zero represent words which are common to all documents i.e. the words are not significant or unique to that document.

# Austen TF-IDF

Create a vector of documents, each document is a novel as one string
```{r}
# Vector of names
titles <- c("Pride and Prejudice", "Sense and Sensibility", "Emma", "Persuasion", "Mansfield Park", "Northanger Abbey")

# nested list of each novel
books <- list(prideprejudice, sensesensibility, emma, persuasion, mansfieldpark,  northangerabbey)
```
```{r}
# map_chr - perform same function over all objects return a character vector
books <- purrr::map_chr(
  # data to map
  books,
  # function to apply
  paste,
  # argument for paste function
  collapse = " "
)
```

Build tibble so we have a lis of words with an identifier of which book each word came from
```{r}
all_books_df <- tibble(
  title = titles,
  text = books
) %>% 
  unnest_tokens(output = word, input = text)

all_books_df
```

TF-IDFs
```{r}
all_books_tf_idf <- all_books_df %>% 
  count(word, title) %>% 
  bind_tf_idf(
    term = word,
    document = title,
    n = n
  ) %>% 
  arrange(desc(tf_idf))

all_books_tf_idf %>% 
  select(word, tf_idf)
```

Word with highest TF-IDF 
```{r}
all_books_tf_idf %>% 
  group_by(title) %>% 
  slice_max(tf_idf, n = 20)
```
We find that mainly names which are unique to each book.

# Ngrams

An n-gram is a sequences of n consecutive words
A way of looking at the relationship between words.
2- and 3- grams are the most common.

```{r}
phrases <- c(
  "here is some text",
  "again more text",
  "text is text"
)
```

```{r}
phrases_df <- tibble(
  phrase = phrases,
  id = 1:length(phrases)
)

phrases_df %>% 
  unnest_tokens(
    bigram, 
    # trigram,
    phrase,
    token = "ngrams",
    n = 2 
    # n = 3
  )
```

# Task
For the book “Pride and Prejudice”, find the top:
bigrams
*note that I have used books list, which has already been processed, instead of the raw Pride and Prejudice file.*

```{r}
pride_bigrams <- tibble(
  title = titles,
  text = books
) %>% 
  filter(title == "Pride and Prejudice") %>% 
  unnest_tokens(
    output = bigram,
    input = text,
    token = "ngrams",
    n = 2
  ) %>% 
  count(bigram, sort = TRUE)
pride_bigrams
```

trigrams
```{r}
pride_trigrams <- tibble(
  title = titles,
  text = books
) %>% 
  filter(title == "Pride and Prejudice") %>% 
  unnest_tokens(
    output = trigram,
    input = text,
    token = "ngrams",
    n = 3
  ) %>% 
  count(trigram, sort = TRUE)
pride_trigrams
```

There are other tokenisation methods available in tidytext

```{r}
book_df <- tibble(
  id = 1:length(prideprejudice),
  text = prideprejudice
) 
```

Remove any stop words from either word of bigrams
```{r}
book_bigrams <- book_df %>% 
  unnest_tokens(
    output = bigram,
    input = text,
    token = "ngrams",
    n = 2
  ) %>% 
  count(bigram, sort = TRUE) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  anti_join(stop_words, by =c("word1" = "word")) %>% 
  anti_join(stop_words, by =c("word2" = "word"))

book_bigrams
```

Unite the columns
```{r}
book_bigrams %>% 
  unite(bigram, word1, word2, sep = " ", remove = FALSE) %>% 
  filter(word1 == "dear")
```

